{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle potential OpenMP library duplication issues, common in some environments\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='TRUE'\n",
    "\n",
    "# Core PyTorch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F # Often imported as F for functional API\n",
    "\n",
    "# Mathematical operations\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# Torchvision for datasets, transforms, and utilities\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "from torch.utils.data import Dataset # Specific import for custom datasets\n",
    "\n",
    "# Image handling and plotting\n",
    "from PIL import Image # For image manipulation, often used with custom datasets\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Automatically select GPU if available, otherwise fall back to CPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\") # Confirm which device is being used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#U-Net architecture for noise prediction in diffusion models with built-in residual connections\n",
    "#1.  nn.Conv2d(in_channels, 64, 3, padding=1), This is for the UNet; first term is no. of feature maps and the next is the time embeddings\n",
    "#2.  nn.Conv2d(in_channels=3, out_channels=64, kernel_size=4, stride=2, padding=1, bias=False). This is for regular CNN architecture\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"U-Net architecture for noise prediction in diffusion models with built-in residual connections, optimized for 128x128 RGB images\"\"\"\n",
    "    def __init__(self, input_channels=3, time_embedding_dim=256):\n",
    "        super().__init__() # Initialize the base PyTorch module.\n",
    "\n",
    "        # --- Shared Utility Layers ---\n",
    "        self.downsample_pool = nn.MaxPool2d(2) # Define max pooling for spatial downsampling.\n",
    "        self.upscale_layer = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True) # Set up bilinear upsampling.\n",
    "        self.activation_fn = nn.ReLU() # Use ReLU as the activation function.\n",
    "\n",
    "        # --- Encoder Path Layers (Downsampling) ---\n",
    "        # Encoder Block 1 (handles 128x128 resolution)\n",
    "        self.encoder1_conv_a = nn.Conv2d(input_channels, 64, 3, padding=1) # First convolution for input features.\n",
    "        self.encoder1_norm_a = nn.BatchNorm2d(64) # Batch normalization after first conv.\n",
    "        self.encoder1_conv_b = nn.Conv2d(64, 64, 3, padding=1) # Second convolution for feature refinement.\n",
    "        self.encoder1_norm_b = nn.BatchNorm2d(64) # Batch normalization after second conv.\n",
    "\n",
    "        # Encoder Block 2 (operates on 64x64 resolution)\n",
    "        self.encoder2_conv_a = nn.Conv2d(64, 128, 3, padding=1) # First convolution in this block.\n",
    "        self.encoder2_norm_a = nn.BatchNorm2d(128) # Batch normalization.\n",
    "        self.encoder2_conv_b = nn.Conv2d(128, 128, 3, padding=1) # Second convolution.\n",
    "        self.encoder2_norm_b = nn.BatchNorm2d(128) # Batch normalization.\n",
    "\n",
    "        # Encoder Block 3 (processes 32x32 resolution at bottleneck)\n",
    "        self.encoder3_conv_a = nn.Conv2d(128, 256, 3, padding=1) # First convolution for deepest features.\n",
    "        self.encoder3_norm_a = nn.BatchNorm2d(256) # Batch normalization.\n",
    "        self.encoder3_conv_b = nn.Conv2d(256, 256, 3, padding=1) # Second convolution.\n",
    "        self.encoder3_norm_b = nn.BatchNorm2d(256) # Batch normalization.\n",
    "\n",
    "        # --- Decoder Path Layers (Upsampling) ---\n",
    "        # Decoder Block 3 (upsamples to 64x64, integrates encoder 2 features)\n",
    "        self.decoder3_conv_a = nn.Conv2d(384, 128, 3, padding=1) # First convolution, input accounts for skip connection.\n",
    "        self.decoder3_norm_a = nn.BatchNorm2d(128) # Batch normalization.\n",
    "        self.decoder3_conv_b = nn.Conv2d(128, 128, 3, padding=1) # Second convolution.\n",
    "        self.decoder3_norm_b = nn.BatchNorm2d(128) # Batch normalization.\n",
    "\n",
    "        # Decoder Block 2 (upsamples to 128x128, integrates encoder 1 features)\n",
    "        self.decoder2_conv_a = nn.Conv2d(192, 64, 3, padding=1) # First convolution, input accounts for skip connection.\n",
    "        self.decoder2_norm_a = nn.BatchNorm2d(64) # Batch normalization.\n",
    "        self.decoder2_conv_b = nn.Conv2d(64, 64, 3, padding=1) # Second convolution.\n",
    "        self.decoder2_norm_b = nn.BatchNorm2d(64) # Batch normalization.\n",
    "\n",
    "        # --- Output Layer ---\n",
    "        self.output_conv = nn.Conv2d(64, input_channels, kernel_size=1) # Final convolution to match original channels.\n",
    "\n",
    "        # --- Time Embedding Components ---\n",
    "        self.time_embedding_dim = time_embedding_dim # Store the dimension for time embeddings.\n",
    "\n",
    "        # Define MLPs for projecting time embeddings to each encoder/decoder block's channel size.\n",
    "        self.time_proj_enc1 = nn.Sequential(nn.Linear(time_embedding_dim, 64), nn.SiLU(), nn.Linear(64, 64)) # MLP for encoder block 1.\n",
    "        self.time_proj_enc2 = nn.Sequential(nn.Linear(time_embedding_dim, 128), nn.SiLU(), nn.Linear(128, 128)) # MLP for encoder block 2.\n",
    "        self.time_proj_enc3 = nn.Sequential(nn.Linear(time_embedding_dim, 256), nn.SiLU(), nn.Linear(256, 256)) # MLP for encoder block 3.\n",
    "        self.time_proj_dec3 = nn.Sequential(nn.Linear(time_embedding_dim, 128), nn.SiLU(), nn.Linear(128, 128)) # MLP for decoder block 3.\n",
    "        self.time_proj_dec2 = nn.Sequential(nn.Linear(time_embedding_dim, 64), nn.SiLU(), nn.Linear(64, 64)) # MLP for decoder block 2.\n",
    "\n",
    "    def get_time_embedding(self, time_step):\n",
    "        \"\"\"Generate sinusoidal time embedding and project through MLPs for each block\"\"\"\n",
    "        half_embedding_size = self.time_embedding_dim // 2 # Calculate half dimension for sin/cos embeddings.\n",
    "        embedding_indices = torch.arange(half_embedding_size, device=time_step.device).float() # Create position indices.\n",
    "        frequency_scales = torch.exp(-math.log(10000) * embedding_indices / half_embedding_size) # Compute frequency bands.\n",
    "        scaled_time = time_step * frequency_scales.unsqueeze(0) # Apply time step to frequency scales.\n",
    "        full_embeddings = torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)], dim=-1) # Concatenate sin and cos for full embedding.\n",
    "\n",
    "        # Project time embedding through specific MLPs and reshape for broadcasting.\n",
    "        emb_e1 = self.time_proj_enc1(full_embeddings).unsqueeze(-1).unsqueeze(-1) # For encoder block 1.\n",
    "        emb_e2 = self.time_proj_enc2(full_embeddings).unsqueeze(-1).unsqueeze(-1) # For encoder block 2.\n",
    "        emb_e3 = self.time_proj_enc3(full_embeddings).unsqueeze(-1).unsqueeze(-1) # For encoder block 3.\n",
    "        emb_d3 = self.time_proj_dec3(full_embeddings).unsqueeze(-1).unsqueeze(-1) # For decoder block 3.\n",
    "        emb_d2 = self.time_proj_dec2(full_embeddings).unsqueeze(-1).unsqueeze(-1) # For decoder block 2.\n",
    "\n",
    "        return {\n",
    "            'enc1': emb_e1, # Return time embedding for encoder block 1.\n",
    "            'enc2': emb_e2, # Return time embedding for encoder block 2.\n",
    "            'enc3': emb_e3, # Return time embedding for encoder block 3.\n",
    "            'dec3': emb_d3, # Return time embedding for decoder block 3.\n",
    "            'dec2': emb_d2 # Return time embedding for decoder block 2.\n",
    "        }\n",
    "\n",
    "    def forward(self, input_image, timestep):\n",
    "        \"\"\"Forward pass through U-Net optimized for 128x128 RGB input with time embeddings at each block\"\"\"\n",
    "        timestep_input = timestep.unsqueeze(-1).float() # Prepare timestep tensor for embedding.\n",
    "        block_time_embeddings = self.get_time_embedding(timestep_input) # Get specific time embeddings for all blocks.\n",
    "\n",
    "        # --- Encoder Path Execution ---\n",
    "        # Encoder Block 1 (initial 128x128 resolution)\n",
    "        enc_out_1 = self.activation_fn(self.encoder1_norm_a(self.encoder1_conv_a(input_image))) # First conv, batch norm, and activation.\n",
    "        enc_out_1 = self.activation_fn(self.encoder1_norm_b(self.encoder1_conv_b(enc_out_1))) # Second conv, batch norm, and activation.\n",
    "        enc_out_1 = enc_out_1 + block_time_embeddings['enc1'] # Add time embedding to features.\n",
    "\n",
    "        # Encoder Block 2 (after first downsampling to 64x64)\n",
    "        pooled_enc_1 = self.downsample_pool(enc_out_1) # Apply max pooling.\n",
    "        enc_out_2 = self.activation_fn(self.encoder2_norm_a(self.encoder2_conv_a(pooled_enc_1))) # First conv, batch norm, activation.\n",
    "        enc_out_2 = self.activation_fn(self.encoder2_norm_b(self.encoder2_conv_b(enc_out_2))) # Second conv, batch norm, activation.\n",
    "        enc_out_2 = enc_out_2 + block_time_embeddings['enc2'] # Add time embedding.\n",
    "\n",
    "        # Encoder Block 3 (deepest features after second downsampling to 32x32)\n",
    "        pooled_enc_2 = self.downsample_pool(enc_out_2) # Apply max pooling.\n",
    "        enc_out_3 = self.activation_fn(self.encoder3_norm_a(self.encoder3_conv_a(pooled_enc_2))) # First conv, batch norm, activation.\n",
    "        enc_out_3 = self.activation_fn(self.encoder3_norm_b(self.encoder3_conv_b(enc_out_3))) # Second conv, batch norm, activation.\n",
    "        enc_out_3 = enc_out_3 + block_time_embeddings['enc3'] # Add time embedding.\n",
    "\n",
    "        # --- Decoder Path Execution (with Skip Connections) ---\n",
    "        # Decoder Block 3 (upsampling from 32x32 to 64x64)\n",
    "        upsampled_dec_3 = self.upscale_layer(enc_out_3) # Upsample features.\n",
    "        decoder_input_3 = torch.cat([upsampled_dec_3, enc_out_2], dim=1) # Concatenate with skip connection from encoder 2.\n",
    "        dec_out_3 = self.activation_fn(self.decoder3_norm_a(self.decoder3_conv_a(decoder_input_3))) # First conv, batch norm, activation.\n",
    "        dec_out_3 = self.decoder3_norm_b(self.decoder3_conv_b(dec_out_3)) # Second conv and batch norm.\n",
    "        dec_out_3 = dec_out_3 + block_time_embeddings['dec3'] # Add time embedding.\n",
    "\n",
    "        # Decoder Block 2 (upsampling from 64x64 to 128x128)\n",
    "        upsampled_dec_2 = self.upscale_layer(dec_out_3) # Upsample features.\n",
    "        decoder_input_2 = torch.cat([upsampled_dec_2, enc_out_1], dim=1) # Concatenate with skip connection from encoder 1.\n",
    "        dec_out_2 = self.activation_fn(self.decoder2_norm_a(self.decoder2_conv_a(decoder_input_2))) # First conv, batch norm, activation.\n",
    "        dec_out_2 = self.decoder2_norm_b(self.decoder2_conv_b(dec_out_2)) # Second conv and batch norm.\n",
    "        dec_out_2 = dec_out_2 + block_time_embeddings['dec2'] # Add time embedding.\n",
    "\n",
    "        # --- Final Output ---\n",
    "        return self.output_conv(dec_out_2) # Return the final output, typically predicted noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BETA_START = 0.0001  # Start value for noise schedule\n",
    "BETA_END = 0.02  # End value for noise schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math # Assuming math is needed for BETA_START/BETA_END calculations if not constants\n",
    "# BETA_START and BETA_END should be defined globally or passed as arguments\n",
    "# Example: BETA_START = 0.0001; BETA_END = 0.02\n",
    "\n",
    "def add_noise_at_timestep(x_start, t, timesteps=1000):\n",
    "    \"\"\"\n",
    "    Adds noise to images at a specific timestep 't' according to the DDPM forward diffusion process.\n",
    "    \"\"\"\n",
    "    noise = torch.randn_like(x_start) # Generate random noise with the same shape as the input image.\n",
    "\n",
    "    # Define and move the noise schedule (betas) to the input tensor's device.\n",
    "    betas = torch.linspace(BETA_START, BETA_END, timesteps, device=x_start.device)\n",
    "    alphas_cumprod = torch.cumprod(1.0 - betas, dim=0) # Compute cumulative product of alphas (alpha_bar).\n",
    "\n",
    "    # Extract coefficients for the given timesteps 't' and reshape for broadcasting.\n",
    "    sqrt_alpha_bar_t = alphas_cumprod[t].sqrt().view(-1, 1, 1, 1) # Coefficient for the original image.\n",
    "    sqrt_one_minus_alpha_bar_t = (1.0 - alphas_cumprod[t]).sqrt().view(-1, 1, 1, 1) # Coefficient for the noise.\n",
    "\n",
    "    # Apply the forward diffusion equation to produce noisy images.\n",
    "    noisy_images = sqrt_alpha_bar_t * x_start + sqrt_one_minus_alpha_bar_t * noise # Linearly combine original image and noise.\n",
    "\n",
    "    return noisy_images, noise # Return the generated noisy image and the specific noise that was added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F # Ensure F is imported for F.mse_loss\n",
    "\n",
    "def diffusion_loss_fn(model, x_start, timesteps=1000):\n",
    "    \"\"\"\n",
    "    Calculates the diffusion loss across multiple timesteps for each image in the batch.\n",
    "    \"\"\"\n",
    "    batch_size = x_start.shape[0] # Get the batch size from the input images.\n",
    "    total_batch_loss = torch.zeros(batch_size, device=x_start.device) # Initialize accumulated loss per image.\n",
    "\n",
    "    # Loop a fixed number of times to average loss over different timesteps.\n",
    "    for _ in range(10): # Sample 10 distinct timesteps for each image to increase training robustness.\n",
    "        # Randomly sample timesteps for each image in the batch.\n",
    "        sampled_timesteps = torch.randint(1, timesteps, (batch_size,), device=x_start.device)\n",
    "\n",
    "        # Generate noisy versions of the original images and retrieve the exact noise added.\n",
    "        noisy_x, true_noise = add_noise_at_timestep(x_start, sampled_timesteps, timesteps)\n",
    "\n",
    "        # Have the model predict the noise given the noisy image and timestep.\n",
    "        predicted_noise = model(noisy_x, sampled_timesteps)\n",
    "\n",
    "        # Calculate the Mean Squared Error (MSE) between predicted and true noise.\n",
    "        # 'reduction='none'' keeps individual losses for each element in the batch, channel, height, width.\n",
    "        per_element_loss = F.mse_loss(predicted_noise, true_noise, reduction='none')\n",
    "        # Average the loss across spatial and channel dimensions to get a single loss per image.\n",
    "        per_image_step_loss = per_element_loss.mean(dim=(1, 2, 3))\n",
    "\n",
    "        total_batch_loss += per_image_step_loss # Add this step's loss to the total for each image.\n",
    "\n",
    "    mean_loss_per_image = total_batch_loss / 10 # Average the accumulated loss over the 10 samples.\n",
    "\n",
    "    return mean_loss_per_image # Return the final average loss per image in the batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),  # Resize all images to 128x128 pixels.\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors (scales pixel values to [0, 1]).\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize pixel values to the range [-1, 1].\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 14630 images.\n",
      "Detected classes (subfolders): ['cat', 'dog', 'wild']\n"
     ]
    }
   ],
   "source": [
    "# Define the root directory of your dataset.\n",
    "# IMPORTANT: For ImageFolder to work, your images must be inside at least one subdirectory.\n",
    "# For example, if your images are directly in 'D:\\Users\\VICTOR\\Desktop\\ADRL\\Assignment 3\\Butterfly dataset',\n",
    "# you should create a subfolder like 'D:\\Users\\VICTOR\\Desktop\\ADRL\\Assignment 3\\Butterfly dataset\\all_images\\'\n",
    "# and move all your butterfly images into 'all_images'.\n",
    "data_dir = '/home/vishwa/data_large/GenAI/animal_face/train' # Adjust this path to your actual image subfolder.\n",
    "\n",
    "# batch size reduced from 32 to 24 for 11GB GPU\n",
    "batch_size = 8\n",
    "\n",
    "# Create the dataset using torchvision.datasets.ImageFolder.\n",
    "# ImageFolder automatically finds images in subdirectories and assigns labels based on folder names.\n",
    "dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "\n",
    "# Create the DataLoader for efficient batching and shuffling during training.\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(f\"Loaded {len(dataset)} images.\") # Display the total number of images found.\n",
    "print(f\"Detected classes (subfolders): {dataset.classes}\") # Show the class names (subdirectory names) ImageFolder found.\n",
    "\n",
    "# Example of how to get a batch:\n",
    "# for images, labels in dataloader:\n",
    "#     print(f\"Batch images shape: {images.shape}\") # Should be (batch_size, 3, 128, 128)\n",
    "#     print(f\"Batch labels: {labels}\") # Labels will correspond to the subfolder index (e.g., 0 for 'all_images')\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500, Average Epoch Loss: 0.0733\n",
      "Epoch 2/500, Average Epoch Loss: 0.0349\n",
      "Epoch 3/500, Average Epoch Loss: 0.0302\n",
      "Epoch 4/500, Average Epoch Loss: 0.0277\n",
      "Epoch 5/500, Average Epoch Loss: 0.0260\n",
      "Epoch 6/500, Average Epoch Loss: 0.0245\n",
      "Epoch 7/500, Average Epoch Loss: 0.0239\n",
      "Epoch 8/500, Average Epoch Loss: 0.0234\n",
      "Epoch 9/500, Average Epoch Loss: 0.0231\n",
      "Epoch 10/500, Average Epoch Loss: 0.0226\n",
      "Epoch 11/500, Average Epoch Loss: 0.0221\n",
      "Epoch 12/500, Average Epoch Loss: 0.0222\n",
      "Epoch 13/500, Average Epoch Loss: 0.0218\n",
      "Epoch 14/500, Average Epoch Loss: 0.0218\n",
      "Epoch 15/500, Average Epoch Loss: 0.0217\n",
      "Epoch 16/500, Average Epoch Loss: 0.0215\n",
      "Epoch 17/500, Average Epoch Loss: 0.0212\n",
      "Epoch 18/500, Average Epoch Loss: 0.0215\n",
      "Epoch 19/500, Average Epoch Loss: 0.0210\n",
      "Epoch 20/500, Average Epoch Loss: 0.0210\n",
      "Epoch 21/500, Average Epoch Loss: 0.0208\n",
      "Epoch 22/500, Average Epoch Loss: 0.0208\n",
      "Epoch 23/500, Average Epoch Loss: 0.0208\n",
      "Epoch 24/500, Average Epoch Loss: 0.0209\n",
      "Epoch 25/500, Average Epoch Loss: 0.0209\n",
      "Epoch 26/500, Average Epoch Loss: 0.0207\n",
      "Epoch 27/500, Average Epoch Loss: 0.0204\n",
      "Epoch 28/500, Average Epoch Loss: 0.0206\n",
      "Epoch 29/500, Average Epoch Loss: 0.0204\n",
      "Epoch 30/500, Average Epoch Loss: 0.0205\n",
      "Epoch 31/500, Average Epoch Loss: 0.0204\n",
      "Epoch 32/500, Average Epoch Loss: 0.0204\n",
      "Epoch 33/500, Average Epoch Loss: 0.0203\n",
      "Epoch 34/500, Average Epoch Loss: 0.0202\n",
      "Epoch 35/500, Average Epoch Loss: 0.0203\n",
      "Epoch 36/500, Average Epoch Loss: 0.0204\n",
      "Epoch 37/500, Average Epoch Loss: 0.0202\n",
      "Epoch 38/500, Average Epoch Loss: 0.0202\n",
      "Epoch 39/500, Average Epoch Loss: 0.0204\n",
      "Epoch 40/500, Average Epoch Loss: 0.0203\n",
      "Epoch 41/500, Average Epoch Loss: 0.0201\n",
      "Epoch 42/500, Average Epoch Loss: 0.0200\n",
      "Epoch 43/500, Average Epoch Loss: 0.0200\n",
      "Epoch 44/500, Average Epoch Loss: 0.0200\n",
      "Epoch 45/500, Average Epoch Loss: 0.0200\n",
      "Epoch 46/500, Average Epoch Loss: 0.0199\n",
      "Epoch 47/500, Average Epoch Loss: 0.0200\n",
      "Epoch 48/500, Average Epoch Loss: 0.0200\n",
      "Epoch 49/500, Average Epoch Loss: 0.0201\n",
      "Epoch 50/500, Average Epoch Loss: 0.0199\n",
      "Epoch 51/500, Average Epoch Loss: 0.0198\n",
      "Epoch 52/500, Average Epoch Loss: 0.0197\n",
      "Epoch 53/500, Average Epoch Loss: 0.0199\n",
      "Epoch 54/500, Average Epoch Loss: 0.0198\n",
      "Epoch 55/500, Average Epoch Loss: 0.0199\n",
      "Epoch 56/500, Average Epoch Loss: 0.0200\n",
      "Epoch 57/500, Average Epoch Loss: 0.0195\n",
      "Epoch 58/500, Average Epoch Loss: 0.0196\n",
      "Epoch 59/500, Average Epoch Loss: 0.0197\n",
      "Epoch 60/500, Average Epoch Loss: 0.0198\n",
      "Epoch 61/500, Average Epoch Loss: 0.0198\n",
      "Epoch 62/500, Average Epoch Loss: 0.0197\n",
      "Epoch 63/500, Average Epoch Loss: 0.0196\n",
      "Epoch 64/500, Average Epoch Loss: 0.0198\n",
      "Epoch 65/500, Average Epoch Loss: 0.0198\n",
      "Epoch 66/500, Average Epoch Loss: 0.0198\n",
      "Epoch 67/500, Average Epoch Loss: 0.0195\n",
      "Epoch 68/500, Average Epoch Loss: 0.0198\n",
      "Epoch 69/500, Average Epoch Loss: 0.0196\n",
      "Epoch 70/500, Average Epoch Loss: 0.0196\n",
      "Epoch 71/500, Average Epoch Loss: 0.0197\n",
      "Epoch 72/500, Average Epoch Loss: 0.0196\n",
      "Epoch 73/500, Average Epoch Loss: 0.0197\n",
      "Epoch 74/500, Average Epoch Loss: 0.0197\n",
      "Epoch 75/500, Average Epoch Loss: 0.0198\n",
      "Epoch 76/500, Average Epoch Loss: 0.0195\n",
      "Epoch 77/500, Average Epoch Loss: 0.0198\n",
      "Epoch 78/500, Average Epoch Loss: 0.0196\n",
      "Epoch 79/500, Average Epoch Loss: 0.0197\n",
      "Epoch 80/500, Average Epoch Loss: 0.0196\n",
      "Epoch 81/500, Average Epoch Loss: 0.0195\n",
      "Epoch 82/500, Average Epoch Loss: 0.0196\n",
      "Epoch 83/500, Average Epoch Loss: 0.0195\n",
      "Epoch 84/500, Average Epoch Loss: 0.0196\n",
      "Epoch 85/500, Average Epoch Loss: 0.0197\n",
      "Epoch 86/500, Average Epoch Loss: 0.0195\n",
      "Epoch 87/500, Average Epoch Loss: 0.0196\n",
      "Epoch 88/500, Average Epoch Loss: 0.0196\n",
      "Epoch 89/500, Average Epoch Loss: 0.0195\n",
      "Epoch 90/500, Average Epoch Loss: 0.0193\n",
      "Epoch 91/500, Average Epoch Loss: 0.0197\n",
      "Epoch 92/500, Average Epoch Loss: 0.0195\n",
      "Epoch 93/500, Average Epoch Loss: 0.0196\n",
      "Epoch 94/500, Average Epoch Loss: 0.0195\n",
      "Epoch 95/500, Average Epoch Loss: 0.0195\n",
      "Epoch 96/500, Average Epoch Loss: 0.0195\n",
      "Epoch 97/500, Average Epoch Loss: 0.0196\n",
      "Epoch 98/500, Average Epoch Loss: 0.0196\n",
      "Epoch 99/500, Average Epoch Loss: 0.0194\n",
      "Epoch 100/500, Average Epoch Loss: 0.0194\n",
      "Epoch 101/500, Average Epoch Loss: 0.0196\n",
      "Epoch 102/500, Average Epoch Loss: 0.0194\n",
      "Epoch 103/500, Average Epoch Loss: 0.0194\n",
      "Epoch 104/500, Average Epoch Loss: 0.0193\n",
      "Epoch 105/500, Average Epoch Loss: 0.0194\n",
      "Epoch 106/500, Average Epoch Loss: 0.0194\n",
      "Epoch 107/500, Average Epoch Loss: 0.0195\n",
      "Epoch 108/500, Average Epoch Loss: 0.0193\n",
      "Epoch 109/500, Average Epoch Loss: 0.0195\n",
      "Epoch 110/500, Average Epoch Loss: 0.0195\n",
      "Epoch 111/500, Average Epoch Loss: 0.0197\n",
      "Epoch 112/500, Average Epoch Loss: 0.0193\n",
      "Epoch 113/500, Average Epoch Loss: 0.0192\n",
      "Epoch 114/500, Average Epoch Loss: 0.0194\n",
      "Epoch 115/500, Average Epoch Loss: 0.0194\n",
      "Epoch 116/500, Average Epoch Loss: 0.0194\n",
      "Epoch 117/500, Average Epoch Loss: 0.0192\n",
      "Epoch 118/500, Average Epoch Loss: 0.0194\n",
      "Epoch 119/500, Average Epoch Loss: 0.0194\n",
      "Epoch 120/500, Average Epoch Loss: 0.0194\n",
      "Epoch 121/500, Average Epoch Loss: 0.0194\n",
      "Epoch 122/500, Average Epoch Loss: 0.0191\n",
      "Epoch 123/500, Average Epoch Loss: 0.0192\n",
      "Epoch 124/500, Average Epoch Loss: 0.0192\n",
      "Epoch 125/500, Average Epoch Loss: 0.0191\n",
      "Epoch 126/500, Average Epoch Loss: 0.0193\n",
      "Epoch 127/500, Average Epoch Loss: 0.0192\n",
      "Epoch 128/500, Average Epoch Loss: 0.0193\n",
      "Epoch 129/500, Average Epoch Loss: 0.0191\n",
      "Epoch 130/500, Average Epoch Loss: 0.0193\n",
      "Epoch 131/500, Average Epoch Loss: 0.0193\n",
      "Epoch 132/500, Average Epoch Loss: 0.0192\n",
      "Epoch 133/500, Average Epoch Loss: 0.0190\n",
      "Epoch 134/500, Average Epoch Loss: 0.0194\n",
      "Epoch 135/500, Average Epoch Loss: 0.0192\n",
      "Epoch 136/500, Average Epoch Loss: 0.0192\n",
      "Epoch 137/500, Average Epoch Loss: 0.0192\n",
      "Epoch 138/500, Average Epoch Loss: 0.0193\n",
      "Epoch 139/500, Average Epoch Loss: 0.0191\n",
      "Epoch 140/500, Average Epoch Loss: 0.0194\n",
      "Epoch 141/500, Average Epoch Loss: 0.0194\n",
      "Epoch 142/500, Average Epoch Loss: 0.0192\n",
      "Epoch 143/500, Average Epoch Loss: 0.0192\n",
      "Epoch 144/500, Average Epoch Loss: 0.0193\n",
      "Epoch 145/500, Average Epoch Loss: 0.0192\n",
      "Epoch 146/500, Average Epoch Loss: 0.0192\n",
      "Epoch 147/500, Average Epoch Loss: 0.0192\n",
      "Epoch 148/500, Average Epoch Loss: 0.0191\n",
      "Epoch 149/500, Average Epoch Loss: 0.0191\n",
      "Epoch 150/500, Average Epoch Loss: 0.0191\n",
      "Epoch 151/500, Average Epoch Loss: 0.0192\n",
      "Epoch 152/500, Average Epoch Loss: 0.0189\n",
      "Epoch 153/500, Average Epoch Loss: 0.0191\n",
      "Epoch 154/500, Average Epoch Loss: 0.0192\n",
      "Epoch 155/500, Average Epoch Loss: 0.0190\n",
      "Epoch 156/500, Average Epoch Loss: 0.0191\n",
      "Epoch 157/500, Average Epoch Loss: 0.0193\n",
      "Epoch 158/500, Average Epoch Loss: 0.0192\n",
      "Epoch 159/500, Average Epoch Loss: 0.0192\n",
      "Epoch 160/500, Average Epoch Loss: 0.0190\n",
      "Epoch 161/500, Average Epoch Loss: 0.0189\n",
      "Epoch 162/500, Average Epoch Loss: 0.0193\n",
      "Epoch 163/500, Average Epoch Loss: 0.0191\n",
      "Epoch 164/500, Average Epoch Loss: 0.0195\n",
      "Epoch 165/500, Average Epoch Loss: 0.0189\n",
      "Epoch 166/500, Average Epoch Loss: 0.0192\n",
      "Epoch 167/500, Average Epoch Loss: 0.0192\n",
      "Epoch 168/500, Average Epoch Loss: 0.0192\n",
      "Epoch 169/500, Average Epoch Loss: 0.0190\n",
      "Epoch 170/500, Average Epoch Loss: 0.0191\n",
      "Epoch 171/500, Average Epoch Loss: 0.0189\n",
      "Epoch 172/500, Average Epoch Loss: 0.0193\n",
      "Epoch 173/500, Average Epoch Loss: 0.0190\n",
      "Epoch 174/500, Average Epoch Loss: 0.0191\n",
      "Epoch 175/500, Average Epoch Loss: 0.0191\n",
      "Epoch 176/500, Average Epoch Loss: 0.0191\n",
      "Epoch 177/500, Average Epoch Loss: 0.0191\n",
      "Epoch 178/500, Average Epoch Loss: 0.0192\n",
      "Epoch 179/500, Average Epoch Loss: 0.0192\n",
      "Epoch 180/500, Average Epoch Loss: 0.0191\n",
      "Epoch 181/500, Average Epoch Loss: 0.0190\n",
      "Epoch 182/500, Average Epoch Loss: 0.0190\n",
      "Epoch 183/500, Average Epoch Loss: 0.0191\n",
      "Epoch 184/500, Average Epoch Loss: 0.0189\n",
      "Epoch 185/500, Average Epoch Loss: 0.0191\n",
      "Epoch 186/500, Average Epoch Loss: 0.0192\n",
      "Epoch 187/500, Average Epoch Loss: 0.0192\n",
      "Epoch 188/500, Average Epoch Loss: 0.0191\n",
      "Epoch 189/500, Average Epoch Loss: 0.0193\n",
      "Epoch 190/500, Average Epoch Loss: 0.0189\n",
      "Epoch 191/500, Average Epoch Loss: 0.0193\n",
      "Epoch 192/500, Average Epoch Loss: 0.0191\n",
      "Epoch 193/500, Average Epoch Loss: 0.0191\n",
      "Epoch 194/500, Average Epoch Loss: 0.0191\n",
      "Epoch 195/500, Average Epoch Loss: 0.0191\n",
      "Epoch 196/500, Average Epoch Loss: 0.0190\n",
      "Epoch 197/500, Average Epoch Loss: 0.0194\n",
      "Epoch 198/500, Average Epoch Loss: 0.0189\n",
      "Epoch 199/500, Average Epoch Loss: 0.0191\n",
      "Epoch 200/500, Average Epoch Loss: 0.0190\n",
      "Epoch 201/500, Average Epoch Loss: 0.0192\n",
      "Epoch 202/500, Average Epoch Loss: 0.0193\n",
      "Epoch 203/500, Average Epoch Loss: 0.0191\n",
      "Epoch 204/500, Average Epoch Loss: 0.0191\n",
      "Epoch 205/500, Average Epoch Loss: 0.0189\n",
      "Epoch 206/500, Average Epoch Loss: 0.0190\n",
      "Epoch 207/500, Average Epoch Loss: 0.0189\n",
      "Epoch 208/500, Average Epoch Loss: 0.0189\n",
      "Epoch 209/500, Average Epoch Loss: 0.0191\n",
      "Epoch 210/500, Average Epoch Loss: 0.0191\n",
      "Epoch 211/500, Average Epoch Loss: 0.0191\n",
      "Epoch 212/500, Average Epoch Loss: 0.0189\n",
      "Epoch 213/500, Average Epoch Loss: 0.0189\n",
      "Epoch 214/500, Average Epoch Loss: 0.0191\n",
      "Epoch 215/500, Average Epoch Loss: 0.0187\n",
      "Epoch 216/500, Average Epoch Loss: 0.0189\n",
      "Epoch 217/500, Average Epoch Loss: 0.0189\n",
      "Epoch 218/500, Average Epoch Loss: 0.0192\n",
      "Epoch 219/500, Average Epoch Loss: 0.0191\n",
      "Epoch 220/500, Average Epoch Loss: 0.0190\n",
      "Epoch 221/500, Average Epoch Loss: 0.0190\n",
      "Epoch 222/500, Average Epoch Loss: 0.0188\n",
      "Epoch 223/500, Average Epoch Loss: 0.0191\n",
      "Epoch 224/500, Average Epoch Loss: 0.0190\n",
      "Epoch 225/500, Average Epoch Loss: 0.0192\n",
      "Epoch 226/500, Average Epoch Loss: 0.0188\n",
      "Epoch 227/500, Average Epoch Loss: 0.0194\n",
      "Epoch 228/500, Average Epoch Loss: 0.0191\n",
      "Epoch 229/500, Average Epoch Loss: 0.0191\n",
      "Epoch 230/500, Average Epoch Loss: 0.0190\n",
      "Epoch 231/500, Average Epoch Loss: 0.0192\n",
      "Epoch 232/500, Average Epoch Loss: 0.0191\n",
      "Epoch 233/500, Average Epoch Loss: 0.0190\n",
      "Epoch 234/500, Average Epoch Loss: 0.0190\n",
      "Epoch 235/500, Average Epoch Loss: 0.0189\n",
      "Epoch 236/500, Average Epoch Loss: 0.0186\n",
      "Epoch 237/500, Average Epoch Loss: 0.0192\n",
      "Epoch 238/500, Average Epoch Loss: 0.0190\n",
      "Epoch 239/500, Average Epoch Loss: 0.0191\n",
      "Epoch 240/500, Average Epoch Loss: 0.0189\n",
      "Epoch 241/500, Average Epoch Loss: 0.0190\n",
      "Epoch 242/500, Average Epoch Loss: 0.0188\n",
      "Epoch 243/500, Average Epoch Loss: 0.0188\n",
      "Epoch 244/500, Average Epoch Loss: 0.0189\n",
      "Epoch 245/500, Average Epoch Loss: 0.0188\n",
      "Epoch 246/500, Average Epoch Loss: 0.0191\n",
      "Epoch 247/500, Average Epoch Loss: 0.0191\n",
      "Epoch 248/500, Average Epoch Loss: 0.0189\n",
      "Epoch 249/500, Average Epoch Loss: 0.0191\n",
      "Epoch 250/500, Average Epoch Loss: 0.0189\n",
      "Epoch 251/500, Average Epoch Loss: 0.0189\n",
      "Epoch 252/500, Average Epoch Loss: 0.0188\n",
      "Epoch 253/500, Average Epoch Loss: 0.0190\n",
      "Epoch 254/500, Average Epoch Loss: 0.0189\n",
      "Epoch 255/500, Average Epoch Loss: 0.0188\n",
      "Epoch 256/500, Average Epoch Loss: 0.0190\n",
      "Epoch 257/500, Average Epoch Loss: 0.0188\n",
      "Epoch 258/500, Average Epoch Loss: 0.0193\n",
      "Epoch 259/500, Average Epoch Loss: 0.0189\n",
      "Epoch 260/500, Average Epoch Loss: 0.0189\n",
      "Epoch 261/500, Average Epoch Loss: 0.0187\n",
      "Epoch 262/500, Average Epoch Loss: 0.0189\n",
      "Epoch 263/500, Average Epoch Loss: 0.0190\n",
      "Epoch 264/500, Average Epoch Loss: 0.0189\n",
      "Epoch 265/500, Average Epoch Loss: 0.0189\n",
      "Epoch 266/500, Average Epoch Loss: 0.0188\n",
      "Epoch 267/500, Average Epoch Loss: 0.0191\n",
      "Epoch 268/500, Average Epoch Loss: 0.0187\n",
      "Epoch 269/500, Average Epoch Loss: 0.0188\n",
      "Epoch 270/500, Average Epoch Loss: 0.0190\n",
      "Epoch 271/500, Average Epoch Loss: 0.0189\n",
      "Epoch 272/500, Average Epoch Loss: 0.0189\n",
      "Epoch 273/500, Average Epoch Loss: 0.0191\n",
      "Epoch 274/500, Average Epoch Loss: 0.0189\n",
      "Epoch 275/500, Average Epoch Loss: 0.0189\n",
      "Epoch 276/500, Average Epoch Loss: 0.0188\n",
      "Epoch 277/500, Average Epoch Loss: 0.0191\n",
      "Epoch 278/500, Average Epoch Loss: 0.0187\n",
      "Epoch 279/500, Average Epoch Loss: 0.0190\n",
      "Epoch 280/500, Average Epoch Loss: 0.0188\n",
      "Epoch 281/500, Average Epoch Loss: 0.0188\n",
      "Epoch 282/500, Average Epoch Loss: 0.0188\n",
      "Epoch 283/500, Average Epoch Loss: 0.0188\n",
      "Epoch 284/500, Average Epoch Loss: 0.0190\n",
      "Epoch 285/500, Average Epoch Loss: 0.0189\n",
      "Epoch 286/500, Average Epoch Loss: 0.0189\n",
      "Epoch 287/500, Average Epoch Loss: 0.0187\n",
      "Epoch 288/500, Average Epoch Loss: 0.0189\n",
      "Epoch 289/500, Average Epoch Loss: 0.0190\n",
      "Epoch 290/500, Average Epoch Loss: 0.0191\n",
      "Epoch 291/500, Average Epoch Loss: 0.0190\n",
      "Epoch 292/500, Average Epoch Loss: 0.0189\n",
      "Epoch 293/500, Average Epoch Loss: 0.0188\n",
      "Epoch 294/500, Average Epoch Loss: 0.0189\n",
      "Epoch 295/500, Average Epoch Loss: 0.0189\n",
      "Epoch 296/500, Average Epoch Loss: 0.0188\n",
      "Epoch 297/500, Average Epoch Loss: 0.0189\n",
      "Epoch 298/500, Average Epoch Loss: 0.0189\n",
      "Epoch 299/500, Average Epoch Loss: 0.0188\n",
      "Epoch 300/500, Average Epoch Loss: 0.0187\n",
      "Epoch 301/500, Average Epoch Loss: 0.0189\n",
      "Epoch 302/500, Average Epoch Loss: 0.0190\n",
      "Epoch 303/500, Average Epoch Loss: 0.0190\n",
      "Epoch 304/500, Average Epoch Loss: 0.0187\n",
      "Epoch 305/500, Average Epoch Loss: 0.0187\n",
      "Epoch 306/500, Average Epoch Loss: 0.0188\n",
      "Epoch 307/500, Average Epoch Loss: 0.0189\n",
      "Epoch 308/500, Average Epoch Loss: 0.0189\n",
      "Epoch 309/500, Average Epoch Loss: 0.0188\n",
      "Epoch 310/500, Average Epoch Loss: 0.0190\n",
      "Epoch 311/500, Average Epoch Loss: 0.0189\n",
      "Epoch 312/500, Average Epoch Loss: 0.0191\n",
      "Epoch 313/500, Average Epoch Loss: 0.0190\n",
      "Epoch 314/500, Average Epoch Loss: 0.0189\n",
      "Epoch 315/500, Average Epoch Loss: 0.0187\n",
      "Epoch 316/500, Average Epoch Loss: 0.0187\n",
      "Epoch 317/500, Average Epoch Loss: 0.0188\n",
      "Epoch 318/500, Average Epoch Loss: 0.0187\n",
      "Epoch 319/500, Average Epoch Loss: 0.0189\n",
      "Epoch 320/500, Average Epoch Loss: 0.0189\n",
      "Epoch 321/500, Average Epoch Loss: 0.0187\n",
      "Epoch 322/500, Average Epoch Loss: 0.0189\n",
      "Epoch 323/500, Average Epoch Loss: 0.0189\n",
      "Epoch 324/500, Average Epoch Loss: 0.0189\n",
      "Epoch 325/500, Average Epoch Loss: 0.0188\n",
      "Epoch 326/500, Average Epoch Loss: 0.0189\n",
      "Epoch 327/500, Average Epoch Loss: 0.0188\n",
      "Epoch 328/500, Average Epoch Loss: 0.0188\n",
      "Epoch 329/500, Average Epoch Loss: 0.0186\n",
      "Epoch 330/500, Average Epoch Loss: 0.0190\n",
      "Epoch 331/500, Average Epoch Loss: 0.0190\n",
      "Epoch 332/500, Average Epoch Loss: 0.0189\n",
      "Epoch 333/500, Average Epoch Loss: 0.0185\n",
      "Epoch 334/500, Average Epoch Loss: 0.0188\n",
      "Epoch 335/500, Average Epoch Loss: 0.0189\n",
      "Epoch 336/500, Average Epoch Loss: 0.0187\n",
      "Epoch 337/500, Average Epoch Loss: 0.0187\n",
      "Epoch 338/500, Average Epoch Loss: 0.0190\n",
      "Epoch 339/500, Average Epoch Loss: 0.0189\n",
      "Epoch 340/500, Average Epoch Loss: 0.0189\n",
      "Epoch 341/500, Average Epoch Loss: 0.0190\n",
      "Epoch 342/500, Average Epoch Loss: 0.0189\n",
      "Epoch 343/500, Average Epoch Loss: 0.0187\n",
      "Epoch 344/500, Average Epoch Loss: 0.0188\n",
      "Epoch 345/500, Average Epoch Loss: 0.0191\n",
      "Epoch 346/500, Average Epoch Loss: 0.0189\n",
      "Epoch 347/500, Average Epoch Loss: 0.0190\n",
      "Epoch 348/500, Average Epoch Loss: 0.0189\n",
      "Epoch 349/500, Average Epoch Loss: 0.0190\n",
      "Epoch 350/500, Average Epoch Loss: 0.0190\n",
      "Epoch 351/500, Average Epoch Loss: 0.0191\n",
      "Epoch 352/500, Average Epoch Loss: 0.0189\n",
      "Epoch 353/500, Average Epoch Loss: 0.0190\n",
      "Epoch 354/500, Average Epoch Loss: 0.0189\n",
      "Epoch 355/500, Average Epoch Loss: 0.0188\n",
      "Epoch 356/500, Average Epoch Loss: 0.0187\n",
      "Epoch 357/500, Average Epoch Loss: 0.0188\n",
      "Epoch 358/500, Average Epoch Loss: 0.0188\n",
      "Epoch 359/500, Average Epoch Loss: 0.0189\n",
      "Epoch 360/500, Average Epoch Loss: 0.0189\n",
      "Epoch 361/500, Average Epoch Loss: 0.0189\n",
      "Epoch 362/500, Average Epoch Loss: 0.0189\n",
      "Epoch 363/500, Average Epoch Loss: 0.0189\n",
      "Epoch 364/500, Average Epoch Loss: 0.0190\n",
      "Epoch 365/500, Average Epoch Loss: 0.0187\n",
      "Epoch 366/500, Average Epoch Loss: 0.0190\n",
      "Epoch 367/500, Average Epoch Loss: 0.0190\n",
      "Epoch 368/500, Average Epoch Loss: 0.0190\n",
      "Epoch 369/500, Average Epoch Loss: 0.0187\n",
      "Epoch 370/500, Average Epoch Loss: 0.0188\n",
      "Epoch 371/500, Average Epoch Loss: 0.0187\n",
      "Epoch 372/500, Average Epoch Loss: 0.0188\n",
      "Epoch 373/500, Average Epoch Loss: 0.0187\n",
      "Epoch 374/500, Average Epoch Loss: 0.0188\n",
      "Epoch 375/500, Average Epoch Loss: 0.0188\n",
      "Epoch 376/500, Average Epoch Loss: 0.0185\n",
      "Epoch 377/500, Average Epoch Loss: 0.0188\n",
      "Epoch 378/500, Average Epoch Loss: 0.0190\n",
      "Epoch 379/500, Average Epoch Loss: 0.0189\n",
      "Epoch 380/500, Average Epoch Loss: 0.0189\n",
      "Epoch 381/500, Average Epoch Loss: 0.0187\n",
      "Epoch 382/500, Average Epoch Loss: 0.0187\n",
      "Epoch 383/500, Average Epoch Loss: 0.0188\n",
      "Epoch 384/500, Average Epoch Loss: 0.0188\n",
      "Epoch 385/500, Average Epoch Loss: 0.0188\n",
      "Epoch 386/500, Average Epoch Loss: 0.0188\n",
      "Epoch 387/500, Average Epoch Loss: 0.0187\n",
      "Epoch 388/500, Average Epoch Loss: 0.0186\n",
      "Epoch 389/500, Average Epoch Loss: 0.0187\n",
      "Epoch 390/500, Average Epoch Loss: 0.0188\n",
      "Epoch 391/500, Average Epoch Loss: 0.0187\n",
      "Epoch 392/500, Average Epoch Loss: 0.0189\n",
      "Epoch 393/500, Average Epoch Loss: 0.0188\n",
      "Epoch 394/500, Average Epoch Loss: 0.0188\n",
      "Epoch 395/500, Average Epoch Loss: 0.0187\n",
      "Epoch 396/500, Average Epoch Loss: 0.0188\n",
      "Epoch 397/500, Average Epoch Loss: 0.0188\n",
      "Epoch 398/500, Average Epoch Loss: 0.0186\n",
      "Epoch 399/500, Average Epoch Loss: 0.0187\n",
      "Epoch 400/500, Average Epoch Loss: 0.0189\n",
      "Epoch 401/500, Average Epoch Loss: 0.0186\n",
      "Epoch 402/500, Average Epoch Loss: 0.0187\n",
      "Epoch 403/500, Average Epoch Loss: 0.0187\n",
      "Epoch 404/500, Average Epoch Loss: 0.0187\n",
      "Epoch 405/500, Average Epoch Loss: 0.0189\n",
      "Epoch 406/500, Average Epoch Loss: 0.0187\n",
      "Epoch 407/500, Average Epoch Loss: 0.0188\n",
      "Epoch 408/500, Average Epoch Loss: 0.0185\n",
      "Epoch 409/500, Average Epoch Loss: 0.0189\n",
      "Epoch 410/500, Average Epoch Loss: 0.0189\n",
      "Epoch 411/500, Average Epoch Loss: 0.0191\n",
      "Epoch 412/500, Average Epoch Loss: 0.0187\n",
      "Epoch 413/500, Average Epoch Loss: 0.0188\n",
      "Epoch 414/500, Average Epoch Loss: 0.0188\n",
      "Epoch 415/500, Average Epoch Loss: 0.0188\n",
      "Epoch 416/500, Average Epoch Loss: 0.0187\n",
      "Epoch 417/500, Average Epoch Loss: 0.0188\n",
      "Epoch 418/500, Average Epoch Loss: 0.0187\n",
      "Epoch 419/500, Average Epoch Loss: 0.0187\n",
      "Epoch 420/500, Average Epoch Loss: 0.0188\n",
      "Epoch 421/500, Average Epoch Loss: 0.0187\n",
      "Epoch 422/500, Average Epoch Loss: 0.0187\n",
      "Epoch 423/500, Average Epoch Loss: 0.0188\n",
      "Epoch 424/500, Average Epoch Loss: 0.0186\n",
      "Epoch 425/500, Average Epoch Loss: 0.0190\n",
      "Epoch 426/500, Average Epoch Loss: 0.0190\n",
      "Epoch 427/500, Average Epoch Loss: 0.0189\n",
      "Epoch 428/500, Average Epoch Loss: 0.0188\n",
      "Epoch 429/500, Average Epoch Loss: 0.0187\n",
      "Epoch 430/500, Average Epoch Loss: 0.0189\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-ca2e872e7c8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# Removed: print(f\"Batch {batch_idx+1}, Loss: {loss.item():.4f}\") # Print batch progress and loss value.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Perform backpropagation to compute gradients.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Update model parameters using the optimizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn # For nn.Module, if not already imported\n",
    "from torch.optim import Adam # For the Adam optimizer\n",
    "\n",
    "# Assuming UNet, add_noise_at_timestep, diffusion_loss_fn,\n",
    "# dataloader, and device (e.g., torch.device('cuda' if torch.cuda.is_available() else 'cpu')) are defined.\n",
    "\n",
    "# Model and Training Setup\n",
    "model = UNet().to(device) # Initialize the UNet model and move it to the specified device.\n",
    "model.train() # Set the model to training mode (enables dropout, batch norm updates).\n",
    "\n",
    "# Define training hyperparameters\n",
    "num_epochs = 500 # Number of complete passes through the dataset.\n",
    "learning_rate = 1e-4 # The rate at which model weights are adjusted during optimization.\n",
    "\n",
    "# Initialize the Adam optimizer\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate) # Create an Adam optimizer instance for model's parameters.\n",
    "\n",
    "# Assuming num_epochs, dataloader, optimizer, diffusion_loss_fn, model, and device are already defined\n",
    "\n",
    "# Training loop implementation\n",
    "for epoch in range(num_epochs): # Loop through each training epoch.\n",
    "    total_epoch_loss = 0.0 # Initialize a variable to accumulate loss for the current epoch.\n",
    "    num_batches = 0 # Initialize a counter for the number of batches in the epoch.\n",
    "\n",
    "    # print(f\"Epoch {epoch+1}/{num_epochs}\") # Removed: Display current epoch progress.\n",
    "\n",
    "    for batch_idx, (images, _) in enumerate(dataloader): # Iterate over batches from the dataloader.\n",
    "        optimizer.zero_grad() # Clear previous gradients before a new backward pass.\n",
    "\n",
    "        images = images.to(device) # Move the current batch of images to the active device.\n",
    "\n",
    "        # Calculate diffusion loss\n",
    "        batch_losses = diffusion_loss_fn(model, images) # Compute loss for each image in the batch.\n",
    "        loss = batch_losses.mean() # Get the average loss across the current batch.\n",
    "        \n",
    "        total_epoch_loss += loss.item() # Add the current batch's loss to the total epoch loss.\n",
    "        num_batches += 1 # Increment the batch counter.\n",
    "\n",
    "        # Removed: print(f\"Batch {batch_idx+1}, Loss: {loss.item():.4f}\") # Print batch progress and loss value.\n",
    "\n",
    "        loss.backward() # Perform backpropagation to compute gradients.\n",
    "        optimizer.step() # Update model parameters using the optimizer.\n",
    "\n",
    "        # Clear GPU cache (optional, for memory management)\n",
    "        if batch_idx % 10 == 0: # Check condition every 10 batches.\n",
    "            torch.cuda.empty_cache() # Release unused GPU memory.\n",
    "    \n",
    "    # Calculate and print the average epoch loss\n",
    "    if num_batches > 0:\n",
    "        avg_epoch_loss = total_epoch_loss / num_batches\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Average Epoch Loss: {avg_epoch_loss:.4f}\")\n",
    "    else:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, No batches processed.\")\n",
    "\n",
    "    # Save model checkpoint\n",
    "    torch.save(model.state_dict(), f'unet_model_epoch_{epoch+1}.pth') # Save the model's learned weights after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_yolo7",
   "language": "python",
   "name": "env_yolo7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
